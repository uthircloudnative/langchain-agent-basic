{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM/00nyDfLQ4Ic5z8zTRkqE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Build Your first Agent\n","\n","## Use Case\n","\n","  - Build a spring boot document reader and online article suggestion agent based on RAG Agentic implementation.\n","\n","### Goal of the Agent\n","\n","  - Agent will accept the user query identify if the question is Spring Boot Documentation specific or not.\n","\n","  - If the query is Technical in nature and Spring Boot specific query then it will get the query specific context information from Chroma DB spcecific DB and make LLM call to summarize the user query.\n","\n","  - If the query is not technical in nature and not specific to Spring Boot then answer I don't know.\n","\n","  - This agent will be extend to do a wen search and identify user question specific articles from specific websites only. This use case is yet to be implemented.\n","\n"],"metadata":{"id":"NAvUoSCgJnVj"}},{"cell_type":"markdown","source":["## Agent Architecture overview\n","\n","\n","### Document Loader Componenet\n","\n","   - As this use case demands RAG based Agent. We will first need to store all the Spring Boot latest Documents into RAG DB.\n","\n","   - For this purpose we will use ChromaDB as Vector Store.\n","\n","   - Before storing data into DB we need to first read the latest Spring Boot documentation from https://docs.spring.io/spring-boot/index.html\n","   URL and load the content into ChromaDB.\n","\n","   - When loading we will load the page content and its source (URL of the page from which the conetent was stored)\n","\n","### The Agent Component\n","\n","   - Then we will create a tool which retrive the context information from this DB for the User query and use that information to make LLM call to finalize the result.\n","\n","### Flow\n","\n","\n","  User --> (Query) --> Agent --> Decide Valid Spring Boot Query\n","                             --> No --> Return\n","                             --> Yes --> Retrive Context and Its source\n","                                     --> Make LLM Call\n","                                     --> Respond to User\n","\n"],"metadata":{"id":"d_P26tB8Ld-q"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"j0xOmVRIxEbK"},"outputs":[],"source":["# RAG Agent Implementation Tech dependencies.\n","\n","# Install Dependencise\n","\n","!pip install -qU langchain\n","!pip install -qU langchain-openai\n","!pip install -qU langchain-community\n","!pip install -qU chromadb\n","!pip install -qU langchain-chroma\n","!pip install -qU nest-asyncio"]},{"cell_type":"markdown","source":["### Tech Stack Pre-Requiste\n","\n","  - Python 3.10+ < Python 4\n","  - LangChain Framework\n","  - ChromaDB\n","  - OpenAI API Key\n","  - LangSmith API Key (Optinal-For Tracing LLM calls)"],"metadata":{"id":"ifyfvYRZVK2r"}},{"cell_type":"markdown","source":["### Technical Implementation\n","\n","#### Loader Component\n","\n","  - First we need to load Spring Boot latest documentation into ChromaDB.\n","\n","  - Before loading into DB we need to read this information from https://docs.spring.io/spring-boot/index.html this official website and seprate the main content from other extra information.\n","\n","  - Our loader component will use Langchain's **RecursiveUrlLoader** to crawl through the index page to deep into other pages of this website and get the content alone removing other nav, header, footer information.\n","\n","  - The current code will go 3 level deep child URL from given parent URL and find all the conent and seperated those conents docuemnts along with its meta data information."],"metadata":{"id":"TWt3SDTaWDKX"}},{"cell_type":"code","source":["# Define OpenAI Embedding model to\n","\n","import os\n","import pprint\n","import nest_asyncio\n","from bs4 import BeautifulSoup\n","\n","from google.colab import userdata\n","\n","from langchain_openai import OpenAIEmbeddings\n","from langchain_chroma import Chroma\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","from langchain_community.document_loaders import RecursiveUrlLoader\n","#from langchain_community.document_loaders.sitemap import SitemapLoader\n","\n","nest_asyncio.apply()\n","\n","os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n","os.environ[\"LANGSMITH_API_KEY\"] = userdata.get('LANGSMITH_API_KEY')\n","\n","def extract_spring_boot_content(html: str) -> str:\n","    soup = BeautifulSoup(html, \"html.parser\")\n","\n","    # Find the article with class \"doc\"\n","    article = soup.find('article', {'class': 'doc'})\n","\n","    if article:\n","        # Remove breadcrumbs and pagination navigation\n","        for element in article.find_all(['nav']):\n","            element.decompose()\n","\n","        # Also remove breadcrumbs-container div if you don't want it\n","        breadcrumbs = article.find('div', {'class': 'breadcrumbs-container'})\n","        if breadcrumbs:\n","            breadcrumbs.decompose()\n","\n","        # Get clean text\n","        return article.get_text(separator='\\n', strip=True)\n","\n","    # Fallback if article not found\n","    return \"\"\n","\n","\n","\n","sitemap_url = \"https://docs.spring.io/spring-boot/index.html\"\n","\n","# loader = SitemapLoader(\n","#     web_path=sitemap_url,\n","#     filter_urls=[\"docs.spring.io\"],\n","#     parsing_function=parse_spring_docs,\n","#     requests_per_second=2  # Be a good citizen to avoid getting blocked\n","# )\n","\n","loader = RecursiveUrlLoader(\n","    url=sitemap_url,\n","    max_depth=3,\n","    extractor=extract_spring_boot_content,\n","    prevent_outside=True,\n","    # Remove link_regex to allow all links within the same domain\n","    # Or use a more permissive pattern that matches the domain\n","    #link_regex=r\"https://docs\\.spring\\.io/spring-boot/.*\",\n","    timeout=10\n",")\n","\n","print(\"Loading Spring Boot 4 documentation pages...\")\n","documents = loader.load()\n","print(f\"Successfully loaded {len(documents)} pages.\")\n","\n","url_set = set()\n","for doc in documents:\n","    print(doc.metadata.get('source'))\n","    if(doc.metadata.get('source') == 'https://docs.spring.io/spring-boot/community.html'):\n","        print(doc.page_content)\n","\n","\n","\n","\n"],"metadata":{"id":"rehTGIr2yvan"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"XVKAKvddXlr2"}},{"cell_type":"markdown","source":["#### Loader Component Continued ...\n","\n","  - Once main content documents are seperated then we need to split them further and then store them into DB.\n","\n","  - To split the documents further we will use Langchain's **RecursiveCharacterTextSplitter** with defined chunk size of 1000 and chun_overlap 0f 200\n","\n","  - Once these documents are split into chunks we will create each chunks respective embeddings (Vector representation of each chinks words)  using OpenAIEmbedding model **text-embedding-3-small** and store those embedding vectors in ChromaDB."],"metadata":{"id":"WiSIX57AYiQJ"}},{"cell_type":"code","source":["# Embed and Load documents into Chroma DB\n","\n","import os\n","import pprint\n","\n","from google.colab import userdata\n","\n","from langchain_openai import OpenAIEmbeddings\n","from langchain_chroma import Chroma\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","\n","embeddings = OpenAIEmbeddings(\n","    model=\"text-embedding-3-small\",\n","    chunk_size=1000\n",")\n","\n","vector_store = Chroma(\n","    collection_name = \"spring_boot_4_document_collection\",\n","    embedding_function = embeddings,\n","    persist_directory = \"./sample_data/spring_boot_4_document_db\"\n",")\n","\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=1000,  # chunk size (characters)\n","    chunk_overlap=200,  # chunk overlap (characters)\n","    add_start_index=True,  # track index in original document\n",")\n","\n","\n","all_splits = text_splitter.split_documents(documents)\n","\n","print(f\"Split blog post into {len(all_splits)} sub-documents.\")\n","\n","# To load 5000 document at once to avoid chromaDB limitation of loading >5,461 documents at once.\n","batch_size=5000\n","# To store VectorIDs after adding documents to vector DB\n","vector_ids = []\n","for i in range(0, len(all_splits),batch_size):\n","    batch_docs = all_splits[i:i+batch_size]\n","    vec_ids = vector_store.add_documents(documents=batch_docs)\n","    vector_ids.extend(vec_ids)\n","\n","vect_id_range = vector_ids[:4]\n","print(vect_id_range)\n","\n","#document_ids = vector_store.add_documents(documents=all_splits)\n","\n","#print(document_ids[:3])"],"metadata":{"id":"e5jo_Z0BojZC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## The Agent Component\n","\n","  - Final steps is to create Agent which uses the Context information which we stored in ChromaDB and make LLM call to answer user query.\n","\n","  - Our agent will have a tool which retrive context information from ChromaDB for the user query. If the query is relevant to Spring Boot 4 and its find the conext information it will return documentation contents along with source (page URL from which that conext information was retrived).\n","\n","  - Here our tool might return multiple source information which match user query text as we are using vectorDB similarity_search with k=3. It will try to identify 3 nearest neighbors (documents) which match user input query text.\n","\n","  - If its found the matching docuemnts from DB all of them will be sent to LLM then LLM will consolidate and provide fianl answer to the user query using this retrieved context information."],"metadata":{"id":"FqrwBFNQ2sQN"}},{"cell_type":"code","source":["from langchain.agents import create_agent\n","from langchain.tools import tool\n","from langchain.chat_models import init_chat_model\n","\n","# 1. Enable tracing (REQUIRED)\n","os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n","\n","# 2. Set the API Key (Correct standard name is LANGCHAIN_API_KEY)\n","os.environ[\"LANGSMITH_API_KEY\"] = userdata.get('LANGSMITH_API_KEY')\n","\n","# 3. (Optional) Group these runs into a specific project name\n","os.environ[\"LANGCHAIN_PROJECT\"] = \"Spring_Boot_Agent_Test\"\n","\n","@tool(response_format=\"content_and_artifact\")\n","def retrive_context(query: str):\n","    \"\"\"\n","    Retrieve information from Spring Boot 4 docs.\n","    Returns text and the specific source URL for each chunk.\n","    \"\"\"\n","    # 1. Get raw results from your database\n","    retrieved_docs = vector_store.similarity_search(query, k=3)\n","\n","    formatted_context = []\n","    for doc in retrieved_docs:\n","        source_url = doc.metadata.get('source', 'No URL available')\n","        snippet = f\"SOURCE: {source_url}\\nCONTENT: {doc.page_content}\"\n","        formatted_context.append(snippet)\n","\n","    content = \"\\n\\n---\\n\\n\".join(formatted_context)\n","\n","    # 3. Return content for LLM and full docs as artifact\n","    return content, retrieved_docs\n","\n","def run_agent_query(query: str):\n","    \"\"\"\n","    Standardized function to handle agent execution.\n","    Easy to call from a CLI, a Web API, or a Colab loop.\n","    \"\"\"\n","    responses = []\n","    for event in agent.stream(\n","        {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n","        stream_mode=\"values\"\n","    ):\n","        # In production, you might return the response rather than just printing\n","        responses.append(event[\"messages\"][-1])\n","\n","    return responses\n","\n","# Agent Definition and declaration\n","\n","\n","\n","tools = [retrive_context]\n","\n","system_prompt=(\n","    \"\"\"\n","    You are an expert on Spring Boot 4.0.1.\n","    Use the provided tool to find context for the user's query.\n","\n","    CRITICAL INSTRUCTION: For every fact or code snippet you provide,\n","    you MUST cite the specific SOURCE URL provided in the context.\n","\n","    Format your response like this:\n","    - Answer text...\n","    - Source: [URL here]\n","\n","    If the query is not related to Spring Boot, say you don't understand.\n","    \"\"\"\n",")\n","\n","model = init_chat_model(\"gpt-4.1\")\n","agent = create_agent(model, tools, system_prompt=system_prompt)\n","\n","\n","# user_query = (\n","#     \"\"\"\n","#      Give me list of Managed Dependency Coordinates in spring boot 4.0.1\n","#     \"\"\"\n","# )\n","\n","# for event in agent.stream(\n","#     {\"messages\":[{\"role\":\"user\", \"content\": user_query}]},\n","#     stream_mode=\"values\"\n","# ):\n","\n","#     event[\"messages\"][-1].pretty_print()\n","\n","# Usage in Colab:\n","result = run_agent_query(input(\"Ask something: \"))\n","for message in result:\n","    message.pretty_print()\n","#print(result)\n","\n","# Usage in Production API:\n","# @app.post(\"/chat\")\n","# def chat(payload: dict):\n","#     return run_agent_query(payload[\"query\"])\n","\n"],"metadata":{"id":"oaD8LyIR2rXg"},"execution_count":null,"outputs":[]}]}